{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "334eUOAEIydt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "- In machine learning, a parameter is a variable that the model learns from the data during the training process. These are the internal settings of the model that are adjusted to make better predictions.\n",
        "- This is different from a hyperparameter (like learning rate or the number of trees in a random forest), which is set by the developer before training begins."
      ],
      "metadata": {
        "id": "gfzVFDLhI416"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        " What does negative correlation mean?\n",
        " - Correlation is a statistical measure that describes the strength and direction of a linear relationship between two variables. It is typically measured by a correlation coefficient, which ranges from -1 to +1\n",
        "    - +1: Perfect positive correlation.\n",
        "\n",
        "    - 0: No correlation.\n",
        "\n",
        "    - -1: Perfect negative correlation.\n",
        "\n",
        " - A negative correlation (a value between 0 and -1) means there is an inverse relationship between two variables. As one variable increases, the other variable tends to decrease.\n",
        " - Example: The amount of time spent studying and the number of mistakes made on a test. As study time (variable A) increases, the number of mistakes (variable B) tends to decrease."
      ],
      "metadata": {
        "id": "XVu9rzMqJE0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        " - Machine Learning (ML) is a subfield of artificial intelligence (AI) where computer systems are given the ability to \"learn\" from data—identifying patterns and making decisions—without being explicitly programmed for every task.\n",
        "\n",
        "- The main components of a typical ML system are:\n",
        "  - Data: The input (features and labels) used to train the model and evaluate its performance.\n",
        "  - Model: The algorithm or architecture (e.g., Linear Regression, Neural Network) that processes the data and makes a prediction.\n",
        "  - Loss Function (or Cost Function): A method for measuring the model's error. It quantifies how \"bad\" the model's prediction was compared to the actual answer.\n",
        "  - Optimizer: The mechanism that \"tunes\" the model's parameters (see Q1) to minimize the loss function. It's how the model actually learns."
      ],
      "metadata": {
        "id": "q-E8pG2BJ28H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "- The loss value is the primary measure of a model's error. It directly tells you how poorly the model is performing on the data.\n",
        "\n",
        "- A high loss value means the model's predictions are far from the actual truth. This is a bad model.\n",
        "\n",
        "- A low loss value means the model's predictions are very close to the actual truth. This is a good model.\n",
        "\n",
        "- During training, the goal is to use the optimizer to adjust the model's parameters until the loss value is as low as possible."
      ],
      "metadata": {
        "id": "dBHbUmMFKQSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "- Continuous Variables: These are numerical variables that can take on any value within a given range. They are measurable.\n",
        "\n",
        "  - Examples: Temperature (30.5°C), height (172.3 cm), price ($54.99).\n",
        "\n",
        "- Categorical Variables: These are variables that represent distinct groups or labels. They are qualitative and have a finite (or fixed) number of possible values.\n",
        "\n",
        "  - Examples: 'Color' (Red, Green, Blue), 'City' (New York, London, Tokyo), 'Yes/No'."
      ],
      "metadata": {
        "id": "aSyhE4xLKokR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        " - Machine learning models are mathematical, so they require numerical input. We cannot feed them text labels like 'Red' or 'Cat'. We must convert these categorical variables into numbers using encoding.\n",
        "\n",
        "\n",
        "- The two most common techniques are:\n",
        "\n",
        "  - Label Encoding: This assigns a unique integer to each category.\n",
        "\n",
        "    - Example: 'Red' = 0, 'Green' = 1, 'Blue' = 2.\n",
        "\n",
        "    - When to use: Best for ordinal data, where the categories have a natural order (e.g., 'Small' < 'Medium' < 'Large'). It's risky for nominal data (like colors) because the model might incorrectly learn that 'Blue' (2) is \"greater than\" 'Green' (1).\n",
        "\n",
        "  - One-Hot Encoding: This creates new binary (0 or 1) columns for each category.\n",
        "\n",
        "    - Example: For a 'Color' feature:\n",
        "\n",
        "    - 'Red' becomes [1, 0, 0]\n",
        "\n",
        "    - 'Green' becomes [0, 1, 0]\n",
        "\n",
        "    - 'Blue' becomes [0, 0, 1]\n",
        "\n",
        "    - When to use: This is the safest and most common method for nominal data (where there is no order), as it avoids creating a false ranking."
      ],
      "metadata": {
        "id": "HMrdgSF6K862"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "- This refers to splitting our main dataset into two separate parts:\n",
        "\n",
        "- Training Set (e.g., 80% of the data): This is the \"textbook\" for the model. The model looks at this data (both the features and the correct answers) to learn the underlying patterns.\n",
        "\n",
        "- Testing Set (e.g., 20% of the data): This is the \"final exam.\" This data is kept separate and is never shown to the model during training. After the model is trained, we use the testing set to evaluate how well it performs on new, unseen data. This gives an honest measure of the model's generalization."
      ],
      "metadata": {
        "id": "XfB8WT7dLK5e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module (a collection of tools) within the Scikit-learn library in Python. It contains essential functions for cleaning, transforming, and preparing data before it is fed into a machine learning model.\n",
        "\n",
        "\n",
        "- Common tools in this module include:\n",
        "\n",
        "  - StandardScaler (for feature scaling)\n",
        "\n",
        "  - MinMaxScaler (for feature scaling)\n",
        "\n",
        "  - OneHotEncoder (for categorical data)\n",
        "\n",
        "  - LabelEncoder (for categorical data)"
      ],
      "metadata": {
        "id": "Ozfe9hr8LSG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "- The Test set is the portion of the dataset that is held back and not used during the model training process. Its sole purpose is to provide an unbiased evaluation of the final, trained model. By making predictions on the test set and comparing them to the known true answers, you can measure the model's accuracy, precision, or other metrics on data it has never seen before."
      ],
      "metadata": {
        "id": "xCNMrYTgLYlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        " How do you approach a Machine Learning problem?\n",
        " - We use the train_test_split function from Scikit-learn's model_selection module.\n",
        "\n",
        " - Here is the standard Python code:"
      ],
      "metadata": {
        "id": "qbj9XKafLdpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assume X is your dataframe of features and y is your target variable (answers)\n",
        "\n",
        "# Define X (your features) and y (your target variable) here using your dataset\n",
        "# Example:\n",
        "# X = your_dataframe[['feature1', 'feature2', ...]]\n",
        "# y = your_dataframe['target_variable']\n",
        "\n",
        "\n",
        "# Split the data: 80% for training, 20% for testing\n",
        "# random_state=42 ensures the split is reproducible (we get the same split every time)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XRa24M3OLrqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- A typical machine learning workflow involves several key steps:\n",
        "\n",
        "- Define the Problem: What are we trying to predict? Is it a classification (e.g., 'Spam' or 'Not Spam') or a regression (e.g., 'House Price') problem?\n",
        "\n",
        "- Gather Data: Collect the raw data needed for the project.\n",
        "\n",
        "- Exploratory Data Analysis (EDA): Analyze the data to understand it. This involves finding missing values, identifying outliers, and visualizing relationships between variables.\n",
        "\n",
        "- Data Preprocessing & Feature Engineering: Clean the data. This includes:\n",
        "\n",
        "  - Handling missing values (e.g., filling or dropping them).\n",
        "\n",
        "  - Encoding categorical variables (e.g., One-Hot Encoding).\n",
        "\n",
        "  - Scaling features (e.g., StandardScaler).\n",
        "\n",
        "  - Creating new features from existing ones.\n",
        "- Model Selection: Choose one or more ML algorithms to try (e.g., Linear Regression, Random Forest, SVM).\n",
        "\n",
        "- Data Splitting: Split the data into training and testing sets.\n",
        "\n",
        "- Model Training: Fit the chosen model(s) to the training data using model.fit().\n",
        "\n",
        "- Model Evaluation: Test the model's performance on the testing data using metrics like accuracy, F1-score, or Mean Squared Error.\n",
        "\n",
        "- Hyperparameter Tuning: Adjust the model's hyperparameters (e.g., learning rate) to find the best possible version of the model.\n",
        "\n",
        "- Deployment: If the model is good enough, deploy it so it can make predictions on new, real-world data."
      ],
      "metadata": {
        "id": "rxD7wdGcS624"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "- EDA (Exploratory Data Analysis) is like \"reading the textbook before taking the test.\" You must perform EDA to:\n",
        "\n",
        "- Understand Your Data: See what features you have, what data types they are (categorical/continuous), and how they are distributed.\n",
        "\n",
        "- Identify Errors: Find missing values, outliers, or incorrect data (e.g., an 'Age' of 500) that need to be cleaned.\n",
        "\n",
        "- Find Relationships: Check for correlations between variables. This helps in feature selection (deciding which variables are important).\n",
        "\n",
        "- Guide Modeling: The insights from EDA (e.g., \"this data is not linear\") help you choose the right type of ML model to use.\n",
        "\n",
        "- Fitting a model to \"dirty\" or un-analyzed data will almost always result in a poor, unreliable model."
      ],
      "metadata": {
        "id": "J31mjxskT413"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "- Correlation is a statistical measure that describes the strength and direction of a linear relationship between two variables. It is typically measured by a correlation coefficient, which ranges from -1 to +1"
      ],
      "metadata": {
        "id": "RkxvY7FFUIay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "- A negative correlation (a value between 0 and -1) means there is an inverse relationship between two variables. As one variable increases, the other variable tends to decrease."
      ],
      "metadata": {
        "id": "5INd1Ce2UVf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "- The easiest way is by using the Pandas library. If we have our data in a DataFrame called df, we just call the .corr() method."
      ],
      "metadata": {
        "id": "ku4Db2d_Uh8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "- Causation: This means that a change in one variable directly causes a change in another.\n",
        "- Difference: Correlation does not imply causation. This is the most important rule in data analysis. Just because two variables move together (correlation) does not mean one is making the other happen (causation)."
      ],
      "metadata": {
        "id": "KzWgYsC-U0ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.\n",
        " What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        " - An Optimizer is the \"engine\" of a machine learning model. Its job is to systematically change the model's parameters (like its weights) with the single goal of minimizing the loss function (i.e., making the model's error as small as possible). It's the core of the \"learning\" process.\n",
        "\n",
        "\n",
        "- Common Types of Optimizers:\n",
        "\n",
        "- Gradient Descent (GD) / Batch GD:\n",
        "\n",
        "  - How it works: It calculates the error (gradient) for the entire training dataset and then takes one step (updates the parameters) in the direction that reduces the error.\n",
        "\n",
        "  - Pro/Con: It's accurate but extremely slow on large datasets because it has to look at every single data point just to make one update.\n",
        "\n",
        "- Stochastic Gradient Descent (SGD):\n",
        "\n",
        "  - How it works: It does the opposite. It calculates the error for only one data point at a time and updates the parameters immediately.\n",
        "\n",
        "  - Pro/Con: It's very fast, but the updates are \"noisy\" and \"jumpy\" because they are based on just one sample. It can be hard to find the perfect minimum.\n",
        "\n",
        "- Mini-Batch Gradient Descent:\n",
        "\n",
        "  - How it works: This is the practical compromise and the most common. It calculates the error for a small batch (e.g., 32 or 64 data points) at a time and updates the parameters.\n",
        "\n",
        "  - Pro/Con: It provides a balance—it's fast like SGD but more stable like Batch GD.\n",
        "\n",
        "- Adam (Adaptive Moment Estimation):\n",
        "\n",
        "  - How it works: An advanced optimizer that is very popular, especially in deep learning. It's a \"smart\" optimizer that adapts the learning rate for each parameter individually.\n",
        "\n",
        "  - Pro/Con: It often converges (finds the minimum loss) much faster and more reliably than other optimizers. It's frequently the default choice."
      ],
      "metadata": {
        "id": "Nmt6lgCqU_HC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model ?\n",
        "- sklearn.linear_model is a module in the Scikit-learn library that contains all the machine learning models based on a linear formula (like y = mx + b).\n",
        "\n",
        "- This module includes:\n",
        "  - LinearRegression: The standard model for regression (predicting a continuous value like price).\n",
        "  - LogisticRegression: Used for classification (predicting a category like 'Yes' or 'No'), despite its name.\n",
        "  - Ridge and Lasso: Advanced types of linear regression that include regularization to prevent overfitting.57"
      ],
      "metadata": {
        "id": "FySJvHQKVv50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "- What it does: model.fit() is the command that starts the training process. It tells the model to look at the training data, learn the patterns, and adjust its internal parameters to minimize the loss.\n",
        "\n",
        "- Arguments: It must be given the training data.\n",
        "\n",
        "- X: The features (the \"inputs\" or \"questions\") of the training set.\n",
        "\n",
        "- y: The target (the \"labels\" or \"answers\") of the training set.\n",
        "\n",
        "  - Example: model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "QwygkOFAWM4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "- What it does: model.predict() is used after the model has been trained (using .fit()). It takes new, unseen data (for which you don't know the answer) and uses the model's learned patterns to generate a prediction.\n",
        "\n",
        "\n",
        "- Arguments: It only needs the features of the new data.\n",
        "\n",
        "- X: The features of the data you want to predict (e.g., X_test).\n",
        "\n",
        "  - Example: predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "c9KGPTlKWeBv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuous and categorical variables?\n",
        "- These are two primary types of data:\n",
        "\n",
        "- Continuous Variables: Numerical data that can be measured. They can take on any value within a range (including fractions/decimals).\n",
        "\n",
        "- Examples: Height, weight, temperature, time.\n",
        "\n",
        "- Categorical Variables: Data that represents groups, labels, or categories. They have a limited number of possible values.\n",
        "\n",
        "- Examples: 'Gender' (Male, Female, Other), 'Payment Method' (Credit Card, PayPal, Cash)."
      ],
      "metadata": {
        "id": "BM_PW0YIW3eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "- What it is: Feature scaling is a preprocessing technique used to standardize the range of features. For example, it can transform all features so they fall between 0 and 1, or so they have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "- How it helps: Many ML algorithms are sensitive to the scale of the data.\n",
        "\n",
        "- Example: Imagine you have two features: 'Age' (range: 20-70) and 'Salary' (range: 50,000-200,000).\n",
        "\n",
        "- The 'Salary' feature has much larger numerical values. An algorithm like k-Nearest Neighbors (k-NN) or an SVM will be dominated by the 'Salary' feature. It will mistakenly believe 'Salary' is more important than 'Age' simply because its numbers are bigger.\n",
        "\n",
        "- Scaling puts all features on a \"level playing field\" (e.g., both 'Age' and 'Salary' are transformed to a range of 0 to 1), so the algorithm treats them fairly and can find the true patterns."
      ],
      "metadata": {
        "id": "DJ31OxDyXGDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "- We use the sklearn.preprocessing module. The two most common methods are StandardScaler and MinMaxScaler.\n",
        "\n",
        "\n",
        "- Example using StandardScaler (Z-score scaling): This scales the data to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "MAgrn9nPXUau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Create the scaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 2. Fit and transform the TRAINING data\n",
        "#    (It learns the mean/std from X_train and then scales it)\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# 3. Only TRANSFORM the TESTING data\n",
        "#    (It uses the mean/std learned from the *training* data to scale the test data)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "1kN_q_HlXgIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Note: It is crucial to fit_transform on the training set but only transform on the test set to prevent \"data leakage\" (peeking at the test set's answers)."
      ],
      "metadata": {
        "id": "vnSg07EuXjkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "- sklearn.preprocessing is a module in the Scikit-learn (sklearn) library in Python. It provides a suite of tools used for data transformation and cleaning before applying a machine learning model. Its main functions are for feature scaling (e.g., StandardScaler), encoding categorical variables (e.g., OneHotEncoder), and normalizing data."
      ],
      "metadata": {
        "id": "7MlJIkyrXmvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "- The standard method is to use the train_test_split function from Scikit-learn's model_selection module."
      ],
      "metadata": {
        "id": "Oi5wNxk4X2ue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "- Data encoding is the process of converting data from one format into another.\n",
        "\n",
        "- In machine learning, this term specifically refers to converting categorical variables (which are text-based or non-numeric) into a numerical representation so that machine learning algorithms can process them.\n",
        "\n",
        "- The most common types are:\n",
        "\n",
        "- Label Encoding: Converts labels into single numbers (e.g., 'Small'=0, 'Medium'=1, 'Large'=2).\n",
        "\n",
        "- One-Hot Encoding: Converts labels into new binary columns (e.g., 'Red' = [1, 0, 0], 'Green' = [0, 1, 0])."
      ],
      "metadata": {
        "id": "6sGbby1RYA0N"
      }
    }
  ]
}